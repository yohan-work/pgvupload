import os, io, hashlib, requests
from typing import List, Optional

from fastapi import FastAPI, UploadFile, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from supabase import create_client, Client

from pypdf import PdfReader
import docx as docx_reader
from pptx import Presentation

load_dotenv()

APP = FastAPI(title="RAG Upload Admin (FastAPI)")

# ===== CORS ì„¤ì • (Nextron ì•± ì—°ë™ìš©) =====
APP.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Nextron ì•±ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== í™˜ê²½ì„¤ì • =====
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY") 
OLLAMA_BASE = os.getenv("OLLAMA_BASE", "http://127.0.0.1:11434")
EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ===== ìœ í‹¸ =====
def extract_text_from_file(file_bytes: bytes, filename: str, mime: Optional[str]) -> str:
    name_lower = (filename or "").lower()
    mime = (mime or "").lower()
    
    # PDF íŒŒì¼
    if name_lower.endswith(".pdf") or "pdf" in mime:
        return extract_pdf_text(file_bytes)
    
    # PowerPoint íŒŒì¼
    if (name_lower.endswith((".ppt", ".pptx")) or 
        "presentation" in mime or 
        "powerpoint" in mime or
        "presentationml" in mime):
        return extract_pptx_text(file_bytes)
    
    # Word íŒŒì¼
    if (name_lower.endswith(".docx") or 
        "wordprocessing" in mime or 
        ("word" in mime and "presentation" not in mime)):
        return extract_docx_text(file_bytes)
    
    # ì¼ë°˜ í…ìŠ¤íŠ¸
    try:
        return file_bytes.decode("utf-8", errors="ignore")
    except:
        return ""

def extract_pdf_text(file_bytes: bytes) -> str:
    text = []
    with io.BytesIO(file_bytes) as buf:
        reader = PdfReader(buf)
        for page in reader.pages:
            t = page.extract_text() or ""
            text.append(t)
    return "\n".join(text).strip()

def extract_docx_text(file_bytes: bytes) -> str:
    with io.BytesIO(file_bytes) as buf:
        doc = docx_reader.Document(buf)
        return "\n".join(p.text for p in doc.paragraphs).strip()

def extract_pptx_text(file_bytes: bytes) -> str:
    """PowerPoint íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    text = []
    with io.BytesIO(file_bytes) as buf:
        prs = Presentation(buf)
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    shape_text = shape.text.strip()
                    if shape_text:
                        text.append(shape_text)
                # í‘œ ì•ˆì˜ í…ìŠ¤íŠ¸ë„ ì¶”ì¶œ
                if shape.has_table:
                    for row in shape.table.rows:
                        for cell in row.cells:
                            cell_text = cell.text.strip()
                            if cell_text:
                                text.append(cell_text)
    return "\n".join(text).strip()

def chunk_text(text: str, size: int, overlap: int) -> List[str]:
    text = text.strip()
    if not text:
        return []
    chunks = []
    start = 0
    N = len(text)
    while start < N:
        end = min(start + size, N)
        chunks.append(text[start:end])
        if end == N:
            break
        start = max(0, end - overlap)
    return [c.strip() for c in chunks if c.strip()]

def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def ollama_embed(texts: List[str]) -> List[List[float]]:
    out = []
    for t in texts:
        r = requests.post(
            f"{OLLAMA_BASE}/api/embeddings",
            json={"model": EMBED_MODEL, "prompt": t},
            timeout=120
        )
        r.raise_for_status()
        out.append(r.json()["embedding"])
    return out

def upsert_to_db(title: str, filename: str, mime: Optional[str], bytes_len: int,
                 whole_text: str, chunks: List[str], embeds: List[List[float]]) -> dict:
    doc_hash = sha256_text(whole_text)
    
    # 1) documents í…Œì´ë¸”ì— upsert
    doc_result = supabase.table("documents").upsert({
        "title": title,
        "source_path": filename,
        "mime_type": mime,
        "bytes": bytes_len,
        "hash": doc_hash
    }, on_conflict="hash").execute()
    
    if not doc_result.data:
        raise Exception("ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨")
    
    document_id = doc_result.data[0]["id"]
    
    # 2) ê¸°ì¡´ chunks ì‚­ì œ
    supabase.table("chunks").delete().eq("document_id", document_id).execute()
    
    # 3) ìƒˆ chunks ì‚½ì… (RPC í•¨ìˆ˜ë¡œ vector ë³€í™˜)
    chunks_data = []
    for idx, (content, embedding) in enumerate(zip(chunks, embeds)):
        chunks_data.append({
            "document_id": document_id,
            "chunk_index": idx,
            "content": content,
            "embedding": embedding  # List[float] ê·¸ëŒ€ë¡œ ì „ë‹¬
        })
    
    # RPC í•¨ìˆ˜ë¡œ embeddingì„ vectorë¡œ ë³€í™˜í•´ì„œ ì‚½ì…
    try:
        result = supabase.rpc("insert_chunks_with_embeddings", {
            "chunks_data": chunks_data
        }).execute()
        
        return {"document_id": str(document_id), "chunks": len(chunks)}
        
    except Exception as e:
        # RPC í•¨ìˆ˜ê°€ ì—†ëŠ” ê²½ìš°, ì„ì‹œë¡œ embeddingì„ JSONìœ¼ë¡œ ì €ì¥
        print(f"RPC í•¨ìˆ˜ ì‚¬ìš© ì‹¤íŒ¨, JSONìœ¼ë¡œ ì €ì¥: {e}")
        
        # embeddingì„ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ì €ì¥ (ì„ì‹œ)
        for chunk_data in chunks_data:
            chunk_data["embedding"] = str(chunk_data["embedding"])
        
        chunk_result = supabase.table("chunks").insert(chunks_data).execute()
        return {"document_id": str(document_id), "chunks": len(chunks)}

# ===== ë·° (1í˜ì´ì§€) =====
FORM_HTML = """
<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8" />
  <title>RAG ì—…ë¡œë“œ ì–´ë“œë¯¼</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial; max-width: 720px; margin: 40px auto; padding: 0 16px; }
    .card { border: 1px solid #e5e7eb; border-radius: 12px; padding: 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    label { display:block; margin:12px 0 6px; font-weight:600; }
    input[type="text"], input[type="file"] { width:100%; padding:10px; border:1px solid #d1d5db; border-radius:8px; }
    button { margin-top:16px; padding:10px 16px; border-radius:10px; border:1px solid #111827; background:#111827; color:white; cursor:pointer; }
    .msg { margin-top:16px; white-space:pre-wrap; font-family:ui-monospace,Menlo,Consolas,monospace; }
    .hint { color:#6b7280; font-size:13px; }
  </style>
</head>
<body>
  <div class="card">
    <h1>RAG ì—…ë¡œë“œ ì–´ë“œë¯¼</h1>
    <form action="/upload" method="post" enctype="multipart/form-data">
      <label>ë¬¸ì„œ ì œëª©</label>
      <input type="text" name="title" placeholder="(ì„ íƒ) ë¯¸ì…ë ¥ ì‹œ íŒŒì¼ëª… ì‚¬ìš©" />
      <label>íŒŒì¼ ì„ íƒ</label>
      <input type="file" name="file" required />
      <div class="hint">PDF / DOCX / PPTX / TXT ì§€ì›</div>
      <button type="submit">ì—…ë¡œë“œ â†’ ì¸ë±ì‹± â†’ DB ì €ì¥</button>
    </form>
    <div class="msg">ì—…ë¡œë“œ í›„ ê²°ê³¼ê°€ ì´ í˜ì´ì§€ë¡œ í‘œì‹œë©ë‹ˆë‹¤.</div>
  </div>
</body>
</html>
"""

# ===== API ì—”ë“œí¬ì¸íŠ¸ (Nextron ì•±ìš©) =====

@APP.get("/api/status")
def api_status():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
        ollama_status = requests.get(f"{OLLAMA_BASE}/api/tags", timeout=5).status_code == 200
        
        # Supabase ì—°ê²° í…ŒìŠ¤íŠ¸  
        supabase_status = bool(supabase.table("documents").select("id").limit(1).execute())
        
        return JSONResponse({
            "status": "healthy",
            "ollama": "connected" if ollama_status else "disconnected",
            "supabase": "connected" if supabase_status else "disconnected",
            "embed_model": EMBED_MODEL,
            "chunk_size": CHUNK_SIZE
        })
    except Exception as e:
        return JSONResponse({
            "status": "error",
            "error": str(e)
        }, status_code=500)

@APP.post("/api/upload")
async def api_upload(file: UploadFile, title: Optional[str] = Form(default=None)):
    """Nextron ì•±ì—ì„œ í˜¸ì¶œí•˜ëŠ” JSON API"""
    try:
        file_bytes = await file.read()
        mime = file.content_type or "application/octet-stream"
        name = file.filename or "uploaded_file"
        title = title.strip() if title else name

        # 1) í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_file(file_bytes, name, mime)
        if not text.strip():
            raise HTTPException(400, f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {name} ({mime})")

        # 2) ì²­í¬
        chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)
        if not chunks:
            raise HTTPException(400, "ì²­í¬ ìƒì„± ì‹¤íŒ¨(ë¹ˆ í…ìŠ¤íŠ¸)")

        # 3) ì„ë² ë”© (Ollama)
        embeds = ollama_embed(chunks)

        # 4) DB ì €ì¥
        result = upsert_to_db(
            title=title,
            filename=name,
            mime=mime,
            bytes_len=len(file_bytes),
            whole_text=text,
            chunks=chunks,
            embeds=embeds
        )
        
        return JSONResponse({
            "success": True,
            "document_id": result["document_id"],
            "title": title,
            "filename": name,
            "mime": mime,
            "bytes": len(file_bytes),
            "chunks": result["chunks"],
            "message": "ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ"
        })
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@APP.get("/api/documents")
def api_list_documents(limit: int = 50, offset: int = 0):
    """ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡"""
    try:
        result = supabase.table("documents") \
            .select("id, title, source_path, mime_type, bytes, created_at") \
            .order("created_at", desc=True) \
            .limit(limit) \
            .offset(offset) \
            .execute()
            
        return JSONResponse({
            "success": True,
            "documents": result.data,
            "count": len(result.data)
        })
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@APP.post("/api/search-vectors")
async def api_search_vectors(request: dict):
    """Nextronìš© ë²¡í„° ë°ì´í„° ì œê³µ API"""
    try:
        query = request.get("query", "")
        
        if not query.strip():
            raise HTTPException(400, "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        # 1) ì§ˆì˜ ì„ë² ë”© ìƒì„±
        query_embedding = ollama_embed([query])[0]  # List[float]
        
        # 2) ëª¨ë“  ì²­í¬ì™€ ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        import json
        
        chunks_data = supabase.table("chunks") \
            .select("id, document_id, chunk_index, content, embedding") \
            .execute()
        
        docs_data = supabase.table("documents") \
            .select("id, title") \
            .execute()
        
        # ë¬¸ì„œ title ë§¤í•‘
        doc_titles = {doc["id"]: doc["title"] for doc in docs_data.data}
        
        # ì²­í¬ ë°ì´í„° ì²˜ë¦¬ (ì„ë² ë”© íŒŒì‹±ë§Œ)
        processed_chunks = []
        for chunk in chunks_data.data:
            try:
                # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if isinstance(chunk["embedding"], str):
                    embedding_str = chunk["embedding"].strip()
                    if embedding_str.startswith('[') and embedding_str.endswith(']'):
                        chunk_embedding = json.loads(embedding_str)
                    else:
                        chunk_embedding = [float(x.strip()) for x in embedding_str.split(',')]
                else:
                    chunk_embedding = chunk["embedding"]
                
                # ì°¨ì› ê²€ì¦
                if len(chunk_embedding) != len(query_embedding):
                    print(f"âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜: query({len(query_embedding)}) vs chunk({len(chunk_embedding)})")
                    continue
                
                processed_chunks.append({
                    "id": chunk["id"],
                    "content": chunk["content"],
                    "embedding": chunk_embedding,  # íŒŒì‹±ëœ ë²¡í„°
                    "document_id": chunk["document_id"],
                    "document_title": doc_titles.get(chunk["document_id"], ""),
                    "chunk_index": chunk["chunk_index"]
                })
                    
            except Exception as parse_error:
                print(f"ì²­í¬ {chunk['id']} íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                continue
        
        print(f"ğŸ” ë²¡í„° ë°ì´í„° ì œê³µ: '{query}' | ì²­í¬ìˆ˜={len(processed_chunks)} | ì¿¼ë¦¬ë²¡í„°ì°¨ì›={len(query_embedding)}")
        
        return JSONResponse({
            "success": True,
            "query": query,
            "query_embedding": query_embedding,  # ì§ˆì˜ ë²¡í„°
            "chunks": processed_chunks,  # ëª¨ë“  ì²­í¬ ë°ì´í„°ì™€ ë²¡í„°
            "count": len(processed_chunks)
        })
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë²¡í„° ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@APP.post("/api/search")  
async def api_search(request: dict):
    """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (ê¸°ì¡´ ë°©ì‹ + ë²¡í„° ì „ìš© ì˜µì…˜)"""
    try:
        query = request.get("query", "")
        top_k = request.get("top_k", 5)
        threshold = request.get("threshold", 0.5)
        vectors_only = request.get("vectors_only", False)  # ìƒˆ ì˜µì…˜
        
        if not query.strip():
            raise HTTPException(400, "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        # vectors_only ëª¨ë“œë©´ ìƒˆë¡œìš´ APIë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        if vectors_only:
            return await api_search_vectors(request)
        
        # 1) ì§ˆì˜ ì„ë² ë”© ìƒì„±
        query_embedding = ollama_embed([query])[0]  # List[float]
        
        # 2) Pythonìœ¼ë¡œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        import json
        import math
        
        # ëª¨ë“  ì²­í¬ì™€ ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        chunks_data = supabase.table("chunks") \
            .select("id, document_id, chunk_index, content, embedding") \
            .execute()
        
        docs_data = supabase.table("documents") \
            .select("id, title") \
            .execute()
        
        # ë¬¸ì„œ title ë§¤í•‘
        doc_titles = {doc["id"]: doc["title"] for doc in docs_data.data}
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for chunk in chunks_data.data:
            try:
                # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if isinstance(chunk["embedding"], str):
                    # "[1.0, 2.0, ...]" í˜•íƒœì˜ ë¬¸ìì—´ì„ íŒŒì‹±
                    embedding_str = chunk["embedding"].strip()
                    if embedding_str.startswith('[') and embedding_str.endswith(']'):
                        chunk_embedding = json.loads(embedding_str)
                    else:
                        # "1.0,2.0,..." í˜•íƒœë©´ split ì‚¬ìš©
                        chunk_embedding = [float(x.strip()) for x in embedding_str.split(',')]
                else:
                    chunk_embedding = chunk["embedding"]
                
                # ì°¨ì› ê²€ì¦
                if len(chunk_embedding) != len(query_embedding):
                    print(f"âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜: query({len(query_embedding)}) vs chunk({len(chunk_embedding)})")
                    continue
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                dot_product = sum(a * b for a, b in zip(query_embedding, chunk_embedding))
                norm_query = math.sqrt(sum(a * a for a in query_embedding))
                norm_chunk = math.sqrt(sum(a * a for a in chunk_embedding))
                similarity = dot_product / (norm_query * norm_chunk) if (norm_query * norm_chunk) > 0 else 0
                
                # ëª¨ë“  ê²°ê³¼ë¥¼ ì¶”ê°€
                similarities.append({
                    "content": chunk["content"],
                    "similarity": float(similarity),
                    "document_id": chunk["document_id"],
                    "document_title": doc_titles.get(chunk["document_id"], ""),
                    "chunk_index": chunk["chunk_index"]
                })
                    
            except Exception as calc_error:
                print(f"ì²­í¬ {chunk['id']} ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {calc_error}")
                continue
        
        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        similarities.sort(key=lambda x: x["similarity"], reverse=True)
        
        # ì„ê³„ê°’ í•„í„°ë§ ë° ìƒìœ„ kê°œ ì„ íƒ
        results = [s for s in similarities if s["similarity"] > threshold][:top_k]
        
        # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        print(f"ğŸ” ê²€ìƒ‰: '{query}' | ì „ì²´={len(similarities)} | ì„ê³„ê°’>{threshold} | ê²°ê³¼={len(results)}")
        if similarities:
            print(f"ğŸ“Š ìœ ì‚¬ë„: {similarities[0]['similarity']:.3f}(ìµœê³ ) ~ {similarities[-1]['similarity']:.3f}(ìµœì €)")
            for i, s in enumerate(similarities[:3]):
                print(f"  {i+1}. {s['similarity']:.3f}: {s['content'][:50]}...")
        
        return JSONResponse({
            "success": True,
            "query": query,
            "results": results,
            "count": len(results)
        })
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")

# ===== ì›¹ í˜ì´ì§€ (ê¸°ì¡´ ìœ ì§€) =====

@APP.get("/", response_class=HTMLResponse)
def index():
    return HTMLResponse(FORM_HTML)

@APP.post("/upload", response_class=HTMLResponse)
async def upload(file: UploadFile, title: Optional[str] = Form(default=None)):
    file_bytes = await file.read()
    mime = file.content_type or "application/octet-stream"
    name = file.filename or "uploaded_file"
    title = title.strip() if title else name

    # 1) í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = extract_text_from_file(file_bytes, name, mime)
    if not text.strip():
        return HTMLResponse(f"<pre>í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {name} ({mime})</pre>", status_code=400)

    # 2) ì²­í¬
    chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)
    if not chunks:
        return HTMLResponse(f"<pre>ì²­í¬ ìƒì„± ì‹¤íŒ¨(ë¹ˆ í…ìŠ¤íŠ¸)</pre>", status_code=400)

    # 3) ì„ë² ë”© (Ollama)
    try:
        embeds = ollama_embed(chunks)
    except Exception as e:
        return HTMLResponse(f"<pre>ì„ë² ë”© ì˜¤ë¥˜: {str(e)}</pre>", status_code=500)

    # 4) DB ì €ì¥
    try:
        res = upsert_to_db(
            title=title,
            filename=name,
            mime=mime,
            bytes_len=len(file_bytes),
            whole_text=text,
            chunks=chunks,
            embeds=embeds
        )
    except Exception as e:
        return HTMLResponse(f"<pre>DB ì €ì¥ ì˜¤ë¥˜: {str(e)}</pre>", status_code=500)

    pretty = f"""
ì—…ë¡œë“œ/ì¸ë±ì‹± ì™„ë£Œ

- title: {title}
- filename: {name}
- mime: {mime}
- bytes: {len(file_bytes)}
- chunks: {res.get('chunks')}

document_id: {res.get('document_id')}
"""
    back = '<p><a href="/">â† ëŒì•„ê°€ê¸°</a></p>'
    return HTMLResponse(f"<pre>{pretty}</pre>{back}")
